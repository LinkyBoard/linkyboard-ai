"""LLM í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‹¤ì œ LLM APIë¥¼ í˜¸ì¶œí•˜ì—¬ Core LLM ì¸í”„ë¼ê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import asyncio
import logging
import sys

from app.core.llm import (
    LLMMessage,
    LLMTier,
    call_with_fallback,
    create_embedding,
    stream_with_fallback,
)
from app.core.llm.observability import langfuse_client

# ë¡œê·¸ ë ˆë²¨ì„ INFOë¡œ ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)


async def test_basic_completion():
    """ê¸°ë³¸ completion í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("[í…ŒìŠ¤íŠ¸ 1] ê¸°ë³¸ LLM í˜¸ì¶œ (LIGHT í‹°ì–´)")
    print("=" * 60)

    try:
        messages = [
            LLMMessage(role="system", content="You are a helpful assistant."),
            LLMMessage(role="user", content="Say 'Hello, World!' in Korean."),
        ]

        result = await call_with_fallback(
            tier=LLMTier.LIGHT,
            messages=messages,
            temperature=0.7,
            max_tokens=100,
        )

        print("\nâœ… LLM í˜¸ì¶œ ì„±ê³µ!")
        print(f"  ëª¨ë¸: {result.model}")
        print(f"  ì‘ë‹µ: {result.content}")
        print(f"  í† í°: {result.input_tokens} in, {result.output_tokens} out")
        print(f"  ì¢…ë£Œ ì´ìœ : {result.finish_reason}")
        return True

    except Exception as e:
        print(f"\nâŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return False


async def test_streaming_completion():
    """ìŠ¤íŠ¸ë¦¬ë° completion í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("[í…ŒìŠ¤íŠ¸ 2] ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ (STANDARD í‹°ì–´)")
    print("=" * 60)

    try:
        messages = [
            LLMMessage(role="system", content="You are a helpful assistant."),
            LLMMessage(
                role="user", content="Count from 1 to 5 in Korean. Be brief."
            ),
        ]

        print("\nğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ:")
        print("  ", end="", flush=True)

        chunk_count = 0
        async for chunk in stream_with_fallback(
            tier=LLMTier.STANDARD,
            messages=messages,
            temperature=0.7,
            max_tokens=100,
        ):
            print(chunk, end="", flush=True)
            chunk_count += 1

        print(f"\n\nâœ… ìŠ¤íŠ¸ë¦¬ë° ì„±ê³µ! (ì´ {chunk_count}ê°œ ì²­í¬)")
        return True

    except Exception as e:
        print(f"\nâŒ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
        return False


async def test_embedding():
    """ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("[í…ŒìŠ¤íŠ¸ 3] ì„ë² ë”© ìƒì„± (EMBEDDING í‹°ì–´)")
    print("=" * 60)

    try:
        text = "This is a test sentence for embedding."
        vector = await create_embedding(text)

        print("\nâœ… ì„ë² ë”© ìƒì„± ì„±ê³µ!")
        print(f"  ì…ë ¥ í…ìŠ¤íŠ¸: {text}")
        print(f"  ë²¡í„° ì°¨ì›: {len(vector)}")
        print(f"  ë²¡í„° ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): {vector[:5]}")
        return True

    except Exception as e:
        print(f"\nâŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return False


async def test_fallback_mechanism():
    """Fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("[í…ŒìŠ¤íŠ¸ 4] Fallback ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦")
    print("=" * 60)

    try:
        messages = [
            LLMMessage(role="system", content="You are a helpful assistant."),
            LLMMessage(
                role="user",
                content="Explain the concept of fallback in one sentence.",
            ),
        ]

        # LIGHT í‹°ì–´: claude-4.5-haiku -> gpt-4.1-mini -> gemini-2.0-flash
        result = await call_with_fallback(
            tier=LLMTier.LIGHT,
            messages=messages,
            temperature=0.7,
            max_tokens=150,
        )

        print("\nâœ… Fallback ë©”ì»¤ë‹ˆì¦˜ ì •ìƒ ì‘ë™!")
        print(f"  ì„ íƒëœ ëª¨ë¸: {result.model}")
        print(f"  ì‘ë‹µ: {result.content[:100]}...")
        print("\n  ğŸ’¡ LIGHT í‹°ì–´ fallback ìˆœì„œ:")
        print("     1. claude-4.5-haiku")
        print("     2. gpt-4.1-mini")
        print("     3. gemini-2.0-flash")
        return True

    except Exception as e:
        print(f"\nâŒ Fallback í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def test_langfuse_tracing():
    """LangFuse íŠ¸ë ˆì´ì‹± í™•ì¸"""
    print("\n" + "=" * 60)
    print("[í…ŒìŠ¤íŠ¸ 5] LangFuse íŠ¸ë ˆì´ì‹± í™•ì¸")
    print("=" * 60)

    if langfuse_client is None:
        print("\nâš ï¸  LangFuse í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("  ì˜µì €ë²„ë¹Œë¦¬í‹° ì—†ì´ LLM í˜¸ì¶œì€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        return True

    print("\nâœ… LangFuse í´ë¼ì´ì–¸íŠ¸ í™œì„±í™”ë¨")
    print(f"  í˜¸ìŠ¤íŠ¸: {langfuse_client.base_url}")
    print("\n  ğŸ’¡ ìœ„ í…ŒìŠ¤íŠ¸ë“¤ì˜ LLM í˜¸ì¶œì´ ìë™ìœ¼ë¡œ íŠ¸ë ˆì´ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("  ğŸ’¡ LangFuse ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤:")
    print(f"     {langfuse_client.base_url}/traces")

    # Flush to ensure all traces are sent
    langfuse_client.flush()
    print("\n  âœ… Trace ì „ì†¡ ì™„ë£Œ (flush)")

    return True


async def run_all_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 60)
    print("ğŸ§ª Core LLM í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print("\nâš ï¸  ì£¼ì˜: ì´ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤ì œ LLM APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤!")
    print("  ìµœì†Œ 1ê°œ ì´ìƒì˜ LLM API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("  (OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY)")

    results = []

    # ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ë“¤
    results.append(await test_basic_completion())
    results.append(await test_streaming_completion())
    results.append(await test_embedding())
    results.append(await test_fallback_mechanism())

    # ë™ê¸° í…ŒìŠ¤íŠ¸
    results.append(test_langfuse_tracing())

    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    passed = sum(results)
    total = len(results)

    print(f"\n  í†µê³¼: {passed}/{total}")
    print(f"  ì‹¤íŒ¨: {total - passed}/{total}")

    if passed == total:
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("\nğŸ‰ Core LLM ì¸í”„ë¼ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        return True
    else:
        print("\nâŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        print("\nê°€ëŠ¥í•œ ì›ì¸:")
        print("  1. LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        print("  2. API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ")
        print("  3. API í• ë‹¹ëŸ‰ ì´ˆê³¼")
        print("  4. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(run_all_tests())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  í…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
